{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQDOt4K1gFob"
   },
   "source": [
    "# Image classification with Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t5tqRJbgFof"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DFpCp67JgFof"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from data_augmentation import get_data_augmentation_layer\n",
    "from plot import plot_patches\n",
    "from vision_transformer import create_vit_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcc5FrNsgFog"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "docpLsgHgFog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZteqzr8gFoh"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NLRWxToogFoh"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "n_transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwCruj75gFoh"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LSjVl7UIgFoh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 18:12:21.497322: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-27 18:12:21.497436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-11-27 18:12:21.916923: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-27 18:12:21.973715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-11-27 18:12:21.989947: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = get_data_augmentation_layer(image_size=image_size)\n",
    "\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP5uaT1zgFoi"
   },
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9MJXUB8agFoi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_patches(x_train=x_train, patch_size=patch_size, image_size=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3pGztIogFoi"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs7uOHtRgFoj"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGvaSsOPgFoj"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x281fff6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier = create_vit_classifier(input_shape=input_shape,\n",
    "                                       num_classes=num_classes,\n",
    "                                       image_size=image_size,\n",
    "                                       patch_size=patch_size,\n",
    "                                       num_patches=num_patches,\n",
    "                                       projection_dim=projection_dim,\n",
    "                                       n_transformer_layers=n_transformer_layers,\n",
    "                                       num_heads=num_heads,\n",
    "                                       transformer_units=transformer_units,\n",
    "                                       mlp_head_units=mlp_head_units)\n",
    "\n",
    "vit_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " patches_2 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 144, 64)     16192       ['patches_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 144, 64)     128         ['patch_encoder_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 144, 64)     66368       ['layer_normalization_17[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 144, 64)      0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 144, 64)     128         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 144, 128)     8320        ['layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 144, 128)     0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 144, 64)      8256        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 144, 64)      0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 144, 64)      0           ['dropout_20[0][0]',             \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 144, 64)     128         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 144, 64)     66368       ['layer_normalization_19[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 144, 64)      0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 144, 64)     128         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 144, 128)     8320        ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 144, 128)     0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 144, 64)      8256        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 144, 64)      0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 144, 64)      0           ['dropout_22[0][0]',             \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 144, 64)     128         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 144, 64)     66368       ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 144, 64)      0           ['multi_head_attention_10[0][0]',\n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 144, 64)     128         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 144, 128)     8320        ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 144, 128)     0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 144, 64)      8256        ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 144, 64)      0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 144, 64)      0           ['dropout_24[0][0]',             \n",
      "                                                                  'add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 144, 64)     128         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 144, 64)     66368       ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 144, 64)      0           ['multi_head_attention_11[0][0]',\n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 144, 64)     128         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 144, 128)     8320        ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 144, 128)     0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 144, 64)      8256        ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 144, 64)      0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 144, 64)      0           ['dropout_26[0][0]',             \n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 144, 64)     128         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 144, 64)     66368       ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 144, 64)      0           ['multi_head_attention_12[0][0]',\n",
      "                                                                  'add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 144, 64)     128         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 144, 128)     8320        ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 144, 128)     0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 144, 64)      8256        ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 144, 64)      0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 144, 64)      0           ['dropout_28[0][0]',             \n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 144, 64)     128         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 144, 64)     66368       ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 144, 64)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 144, 64)     128         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 144, 128)     8320        ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 144, 128)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 144, 64)      8256        ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 144, 64)      0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 144, 64)      0           ['dropout_30[0][0]',             \n",
      "                                                                  'add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 144, 64)     128         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 144, 64)     66368       ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 144, 64)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 144, 64)     128         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 144, 128)     8320        ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 144, 128)     0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 144, 64)      8256        ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 144, 64)      0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 144, 64)      0           ['dropout_32[0][0]',             \n",
      "                                                                  'add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 144, 64)     128         ['add_29[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 144, 64)     66368       ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 144, 64)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 144, 64)     128         ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 144, 128)     8320        ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 144, 128)     0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 144, 64)      8256        ['dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 144, 64)      0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 144, 64)      0           ['dropout_34[0][0]',             \n",
      "                                                                  'add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 144, 64)     128         ['add_31[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 9216)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 2048)         18876416    ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 2048)         0           ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 1024)         2098176     ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 1024)         0           ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 100)          102500      ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,759,019\n",
      "Trainable params: 21,759,012\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tgkAKP0wgFoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 18:21:13.163403: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit_classifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [15], line 23\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     17\u001b[0m     checkpoint_filepath,\n\u001b[1;32m     18\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[1;32m     33\u001b[0m _, accuracy, top_5_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g591UpwDgFoj"
   },
   "source": [
    "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
    "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
    "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
    "\n",
    "Note that the state of the art results reported in the\n",
    "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
    "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
    "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
    "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
    "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
    "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
    "In practice, it's recommended to fine-tune a ViT model\n",
    "that was pre-trained using a large, high-resolution dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_classification_with_vision_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
